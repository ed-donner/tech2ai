{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
   "metadata": {},
   "source": [
    "# Segment 1 Lab 2\n",
    "\n",
    "## Making our own Customer Support Chatbot\n",
    "\n",
    "One of the most common business use cases of Gen AI.\n",
    "\n",
    "We'll even make our own User Interface - no frontend skills required!\n",
    "\n",
    "We will use the delightful `gradio` framework which makes it remarkably easy for data scientists to build great UIs.\n",
    "\n",
    "We are going to move quickly as this is just a teaser - but please come back and look at this later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231605aa-fccb-447e-89cf-8b187444536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541d58e-2297-4de1-b1f7-77da1b98b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "\n",
    "openai = OpenAI()\n",
    "MODEL = 'gpt-4.1-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16839b5-c03b-4d9d-add6-87a0f6f37575",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7",
   "metadata": {},
   "source": [
    "Reminder of the structure of prompt messages to OpenAI, now showing the response:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73eb80-c1ec-465e-8d8a-f57e75df5cba",
   "metadata": {},
   "source": [
    "## The chat function\n",
    "\n",
    "In order to use Gradio's out-of-the-box Chat User Interface, we need to write a single function, `chat(message, history)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [system_message] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    print(f\"Messages:\\n{messages}\\n\\n\")\n",
    "    results = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    return results.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334422a-808f-4147-9c4c-57d63d9780d0",
   "metadata": {},
   "source": [
    "## And then enter Gradio's magic!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34012ed-f4ec-4983-9498-e4cf45f4b9cb",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb98b86e-09b7-403c-baee-b8f9644a274c",
   "metadata": {},
   "source": [
    "## The illusion of memory\n",
    "\n",
    "Notice something about the messages printed above:\n",
    "\n",
    "We've written this so that every call to the LLM contains the entire conversation so far.\n",
    "\n",
    "And that's super important.\n",
    "\n",
    "Every call to an LLM is entirely stateless; it has no knowledge that we're in the middle of a conversation about a trip to London.\n",
    "\n",
    "**The fact that it appears to maintain the context of the conversation is an illusion; a result of us passing in the entire conversation with every call.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd26c1-47fd-4575-903f-df36d6986d96",
   "metadata": {},
   "source": [
    "# Project - Airline AI Assistant\n",
    "\n",
    "We'll now extend this to make an AI Customer Support assistant for an Airline\n",
    "\n",
    "Let's use the system prompt for 2 purposes:\n",
    "- Set the character\n",
    "- Add some expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3545b4d9-7a39-48be-8140-75d6272695f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message[\"content\"] =  \"\"\"You are a helpful assistant for an Airline called FlightAI.\n",
    "Give concise, humorous, witty, slightly snarky answers, no more than 1 sentence.\n",
    "For context, a return flight to London costs $600.\n",
    "Always be accurate. If you don't know the answer, say so.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b465a-afdb-4d57-bb95-fccb036baba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc005e6d-174b-4d4a-a9d7-6b7648b27b26",
   "metadata": {},
   "source": [
    "# Let's go multi-modal!!\n",
    "\n",
    "We can use DALL-E-3, the image generation model behind GPT-4o, to make us some images\n",
    "\n",
    "Let's put this in a function called artist.\n",
    "\n",
    "### Price alert: each time I generate an image it costs about 4c - don't go crazy with images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae95c0e-db8d-4631-b3bb-353ef13baf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports for handling images\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc7959-9d5b-4b69-93ab-f12d57d9cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def artist(city):\n",
    "    prompt = f\"An image representing a vacation in {city}, showing tourist spots and everything unique about {city}, in a vibrant pop-art style\"\n",
    "    image_response = openai.images.generate(\n",
    "            model=\"dall-e-3\",\n",
    "            prompt=prompt,\n",
    "            size=\"1024x1024\",\n",
    "            n=1,\n",
    "            response_format=\"b64_json\"\n",
    "        )\n",
    "    image_base64 = image_response.data[0].b64_json\n",
    "    image_data = base64.b64decode(image_base64)\n",
    "    return Image.open(BytesIO(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae324773-7c3c-4ef5-9174-de96a247cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = artist(\"New York City\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb4110d-15cf-4f7f-9ed6-411f83a21d08",
   "metadata": {},
   "source": [
    "# Bringing it together\n",
    "\n",
    "This is the start of what you might call an \"agent framework\", in that we will use multiple LLM calls to solve a complex problem.\n",
    "\n",
    "We'll work on a full agent framework in the final project today!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7d62e-2984-4148-984b-2b729b64997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(history):\n",
    "    message = history[-1][\"content\"]\n",
    "    messages = [system_message] + history\n",
    "    results = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    image = artist(\"London\") if \"london\" in message.lower() else None\n",
    "    response = results.choices[0].message.content\n",
    "    history += [{\"role\":\"assistant\", \"content\":response}]\n",
    "    return history, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a600e-1f6d-4fb0-b196-6be23181c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More involved Gradio code as we're not using the preset Chat interface\n",
    "\n",
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "        image_output = gr.Image(height=500)\n",
    "    with gr.Row():\n",
    "        entry = gr.Textbox(label=\"Chat with our AI Assistant:\")\n",
    "\n",
    "    def do_entry(message, history):\n",
    "        history += [{\"role\":\"user\", \"content\":message}]\n",
    "        return \"\", history\n",
    "\n",
    "    entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(\n",
    "        chat, inputs=chatbot, outputs=[chatbot, image_output]\n",
    "    )\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bb6218-2f43-4049-8954-43e60e275a53",
   "metadata": {},
   "source": [
    "# A multi-modal customer support chatbot - in minutes!\n",
    "\n",
    "This illustrated how easy it is to build a chatbot with character and knowledge.\n",
    "\n",
    "# Exercise\n",
    "\n",
    "Take this further - have it generate audio for its responses, and use Tools to look up costs of flights.  \n",
    "See my companion repo llm_engineering in week2 folder for the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bde540-0513-4ae0-8fbd-ffe8ff688087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
